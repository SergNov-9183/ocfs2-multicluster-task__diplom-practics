# ocfs2-multicluster-task__diplom-practics

## Основные понятия

### OCFS2 (Oracle Cluster File System 2)

Кластерная файловая система: несколько узлов (серверов или контейнеров) одновременно монтируют одну и ту же ФС на одном устройстве. Ядро OCFS2 координирует доступ через **DLM** (Distributed Lock Manager), чтобы не было конфликтов при записи.

### Heartbeat (пульс, «сердцебиение»)

- **Назначение:** узлы кластера должны знать, кто «жив», а кто отвалился. Для этого каждый узел периодически пишет в специальную область на диске (или в файл) - это и есть **heartbeat-регион**.
- **Регион** - это либо UUID существующей OCFS2-ФС на устройстве, либо путь к файлу/устройству, который все узлы используют как общую «точку пульса».
- Если узел перестаёт писать в регион, остальные считают его мёртвым и освобождают его блокировки (DLM). Без heartbeat кластер не сможет корректно работать при сбоях узлов.

### o2cb (OCFS2 Cluster Base)

Пользовательский демон и утилиты для стека **o2cb**: регистрация кластера, настройка heartbeat-режима (local/global), добавление/удаление heartbeat-регионов, запуск/остановка heartbeat. Команды: `o2cb register-cluster`, `o2cb add-heartbeat`, `o2cb start-heartbeat`, `o2cb cluster-status` и т.д.

### DRBD (Distributed Replicated Block Device)

В текущем сценарии DRBD используется **на одной машине** (single-node): один backing-файл (образ диска) привязывается к loop-устройству, поверх поднимается ресурс DRBD и создаётся устройство `/dev/drbd0`. Все контейнеры получают это же устройство через `--device /dev/drbd0:/dev/drbd0`. То есть DRBD даёт **общее блочное устройство** для всех контейнеров, а не репликацию между серверами.

### Bootstrap и Join

- **Bootstrap (узел 1):** создаёт конфиг кластера (`cluster.conf`), добавляет единственный heartbeat-регион, запускает heartbeat. Это «первый» узел, который инициализирует кластер.
- **Join (узлы 2..N):** получают уже готовый `cluster.conf` (скопированный с узла 1), только регистрируют кластер и запускают heartbeat по уже прописанному региону. Регион не добавляют заново.

---

### 1. Проверки и подготовка

- **ensure_docker** - проверка, что Docker запущен и доступен.
- **ensure_host_drbd9** - проверка, что на хосте есть DRBD 9.x (`/proc/drbd`), при необходимости загрузка модулей `drbd`, `drbd_transport_tcp`.
- **ensure_clean_drbd_minors** - если уже есть устройства drbd (например, drbd0), скрипт пытается переиспользовать существующий ресурс или очистить его (down, del-minor, del-resource), чтобы не было конфликтов.

### 2. Сеть и образ

- **create_network** - создаётся Docker-сеть `ocfs2-network` с подсетью 172.20.0.0/16; у каждого контейнера будет свой IP (172.20.0.11, 172.20.0.12, …).
- **build_image** - сборка образа `ocfs2-node:latest` по `Dockerfile.ocfs2` (утилиты OCFS2, скрипты setup/run/collect и т.д.).

### 3. DRBD на хосте

- **host_setup_drbd_single** - поднимается DRBD в режиме «один узел»:
  - создаётся backing-файл `/var/lib/drbd/ocfs2-resource_backing.img` (по умолчанию 4 GiB);
  - файл привязывается к loop-устройству;
  - пишется конфиг ресурса в `/etc/drbd.d/ocfs2-resource.res`;
  - выполняется `drbdadm create-md`, `drbdadm up`, `drbdadm primary`;
  - в итоге на хосте появляется `/dev/drbd0`, доступное для контейнеров.

### 4. Настройка кластера OCFS2

**configure_ocfs2_cluster** делает следующее:

1. **Остановка и удаление контейнеров** - чтобы ни один процесс не держал модули OCFS2 и устройство DRBD.
2. **Остановка o2cb на хосте** - `pkill o2cb`, `o2cb stop-heartbeat`, `o2cb unregister-cluster` (если o2cb установлен на хосте).
3. **Размонтирование OCFS2 на хосте** - если что-то было смонтировано по типу ocfs2.
4. **Выгрузка модулей OCFS2 на хосте** - `modprobe -r` для ocfs2_stack_o2cb, ocfs2_dlm, ocfs2_dlmfs, ocfs2, ocfs2_nodemanager, ocfs2_stackglue. Это нужно, чтобы убить старые kernel-потоки `[o2hb-XXX]`, которые не исчезают без выгрузки модуля.
5. **Проверка:** если модули не выгрузились и в системе есть потоки `[o2hb-*]`, скрипт выдаёт ошибку и советует перезагрузить хост - без перезагрузки избавиться от «залипших» o2hb нельзя.
6. **Проверка наличия /dev/drbd0** - если устройства нет, вызывается снова `host_setup_drbd_single`.
7. **Обнуление начала /dev/drbd0** - `wipefs -a` и `dd if=/dev/zero` на первые мегабайты (кроме последних 128 MiB, где у DRBD с internal-метаданными лежит метаданные). Нужно, чтобы на устройстве не осталось старых подписей ФС и «мусора», который библиотека может принять за OCFS1.
8. **Создание контейнеров заново** - `create_containers`: N контейнеров с именами ocfs2-node-1 … ocfs2-node-N, с общей сетью, `--privileged`, `--device /dev/drbd0:/dev/drbd0`, образ ocfs2-node:latest, в качестве процесса `tail -f /dev/null`.
9. **Bootstrap на узле 1:** внутри контейнера ocfs2-node-1 запускается `setup_ocfs2_cluster.sh CLUSTER_NAME NODES bootstrap`. Этот скрипт:
   - создаёт `/etc/ocfs2/cluster.conf` (узлы, IP, имя кластера);
   - загружает модули OCFS2 и поднимает o2cb;
   - очищает старые heartbeat-регионы (в subshell с `set +e`, чтобы падения o2cb не роняли скрипт);
   - регистрирует кластер;
   - пытается добавить heartbeat-регион: сначала на `/dev/drbd0` (local mode), при неудаче - fallback на global mode с путём к файлу `/tmp/o2hb_region.img` (чтобы избежать ошибки «Unknown code ocfs 8» при открытии блочного устройства через libocfs2);
   - запускает heartbeat.
10. **Проверка cluster.conf** - на хосте копируется `cluster.conf` из узла 1 и проверяется, что в нём есть секция `heartbeat:` (иначе add-heartbeat не сработал).
11. **Join на узлах 2..N:** на каждый контейнер копируется тот же `cluster.conf`, затем в каждом выполняется `setup_ocfs2_cluster.sh ... join` (только register + start-heartbeat, без add-heartbeat).
12. Ожидание синхронизации и проверка `o2cb cluster-status` на всех узлах.

### 5. Создание файловой системы

- **create_filesystem** - в контейнере ocfs2-node-1 выполняется `mkfs.ocfs2` по `/dev/drbd0`: кластерное имя, число слотов по числу узлов, метка «ocfs2vol». ФС создаётся один раз и потом используется всеми узлами.

### 6. Монтирование на всех узлах

- **mount_fs_all_nodes** - на каждом контейнере проверяется, что кластер онлайн (`o2cb cluster-status`), затем `mount -t ocfs2 /dev/drbd0 /mnt/ocfs2`. Несколько попыток при сбое.

### 7. Тесты и отчёты

- **run_tests** - на каждом узле в фоне запускается `run_tests.sh` (запись/чтение в общую точку монтирования), результаты пишутся в логи на хосте.
- **collect_reports** - собираются отчёты GCOV: с хоста - ядро (OCFS2) из `/sys/kernel/debug/gcov`, из контейнеров - покрытие ocfs2-tools; всё складывается в каталог `gcov_reports_YYYYMMDD_HHMMSS/` (тесты, kernel_html, tools_html).

---

## Режим cleanup

При вызове `./deploy_ocfs2_cluster.sh cleanup`:

- удаляются все контейнеры ocfs2-node-*;
- удаляется сеть ocfs2-network;
- на хосте останавливаются o2cb, размонтируется ocfs2, выгружаются модули OCFS2;
- выполняется **cleanup_host_drbd**: остановка ресурса DRBD, удаление ресурса, отвязка loop от backing-файла, при необходимости перезагрузка модуля drbd.
